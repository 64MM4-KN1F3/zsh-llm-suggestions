[x] Look into utilising prompt caching for mlx_lm to speed inference: mlx_lm/examples/chat.py
[x] Specify optional python packages (openai, mlx_lm etc) correctly in pyproject.toml
[x] Update readme to specify how to add required optional packages for use case(s)
[x] Update MLX prompt cache location to be dynamically set
[x] Catch ollama error at line 76 in zsh-llm-suggestions-ollama.py in case that Ollama is not running
[ ] Possibly add an installer script as QoL improvement.