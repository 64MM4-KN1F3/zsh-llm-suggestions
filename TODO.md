[x] Look into utilising prompt caching for mlx_lm to speed inference: mlx_lm/examples/chat.py
[x] Specify optional python packages (openai, mlx_lm etc) correctly in pyproject.toml
[x] Update readme to specify how to add required optional packages for use case(s)
[ ] Possibly add an installer script as QoL improvement.